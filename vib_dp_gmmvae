import os
import json
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import torch
from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
from torch.nn import functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import BayesianGaussianMixture
from tqdm import tqdm
import warnings

#####################################
# DEVICE CONFIGURATION
#####################################
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

#####################################
# DATA LOADING & PREPROCESSING
#####################################
def load_and_preprocess_data(test_path, radar_folder, gnd_folder):
    """
    Loads radar.ftr and gnd.ftr for a given test case.
    Assumes radar.ftr file is named as testcase_name.ftr in radar_folder,
    and ground truth file is named as testcase_name_gnd.ftr in gnd_folder.
    Returns:
      - A PyTorch TensorDataset of normalized features (for clustering),
      - List of feature columns for clustering,
      - The original radar DataFrame (dfRadar),
      - The ground truth DataFrame (dfGND).
    For clustering, we remove kinematic quantities (Range, VrelRad, variances)
    and keep quality signals plus Azimuth and Elevation Angle.
    """
    # Construct file paths
    test_name = os.path.splitext(os.path.basename(test_path))[0]
    radar_file = os.path.join(radar_folder, test_name + ".ftr")
    gnd_file = os.path.join(gnd_folder, test_name + "_gnd.ftr")
    
    dfRadar = pd.read_feather(radar_file)
    dfGND = pd.read_feather(gnd_file)
    
    # For clustering, use only quality signals and angles
    clustering_feature_cols = [
        "SNR", "RCS", "aa", "bb", "cc", "dd", "ee", "ff", "gg",
        "hh", "jj", "kk", "Azimuth", "Elevation Angle"
    ]
    
    data = dfRadar[clustering_feature_cols].values.astype(np.float32)
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    dataset = TensorDataset(torch.tensor(data_scaled, dtype=torch.float32))
    
    return dataset, clustering_feature_cols, dfRadar, dfGND

#####################################
# VIB+VAE MODEL DEFINITION (Shared Encoder)
#####################################
class VIBVAE(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim=128):
        super(VIBVAE, self).__init__()
        # Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_mean = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        # Decoder
        self.fc_dec1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_dec2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, input_dim)
    
    def encode(self, x):
        h = F.relu(self.fc1(x))
        h = F.relu(self.fc2(h))
        mu = self.fc_mean(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = F.relu(self.fc_dec1(z))
        h = F.relu(self.fc_dec2(h))
        recon = self.fc_out(h)
        return recon
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar

def vae_loss(x, recon, mu, logvar):
    recon_loss = F.mse_loss(recon, x, reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_loss

def train_vae(model, dataset, batch_size=1024, lr=1e-3, epochs=10, device=device):
    model.to(device)
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
    for epoch in tqdm(range(epochs), desc="Training VAE on GPU"):
        total_loss = 0.0
        for batch in loader:
            x = batch[0].to(device)
            optimizer.zero_grad()
            recon, mu, logvar = model(x)
            loss = vae_loss(x, recon, mu, logvar)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        # Optionally print average loss
        # print(f"Epoch {epoch+1}: Loss = {total_loss/len(dataset):.4f}")
    model.to('cpu')
    return model

#####################################
# ENCODE DATA & OUTLIER DETECTION
#####################################
def encode_data_and_find_outliers(model, dataset, outlier_percentile=75):
    """
    Encodes all data using the trained VAE.
    Computes reconstruction errors and flags outliers as those with error above the percentile threshold.
    Returns:
      - latent_mu: latent representation (mu) for all samples,
      - recon_errors: per-sample reconstruction error,
      - outlier_idx: indices of samples with error > threshold,
      - threshold: the chosen error threshold.
    """
    model.eval()
    loader = DataLoader(dataset, batch_size=1024, shuffle=False, pin_memory=True)
    latent_list = []
    error_list = []
    with torch.no_grad():
        for batch in loader:
            x = batch[0]
            recon, mu, logvar = model(x)
            latent_list.append(mu.numpy())
            errors = ((recon.numpy() - x.numpy())**2).mean(axis=1)
            error_list.append(errors)
    latent_mu = np.concatenate(latent_list, axis=0)
    recon_errors = np.concatenate(error_list, axis=0)
    threshold = np.percentile(recon_errors, outlier_percentile)
    # Drop rows with NaN if they are < 5%, otherwise try to re-run (here we simply drop if <5%)
    nan_mask = np.isnan(latent_mu).any(axis=1)
    frac_nan = np.mean(nan_mask)
    if frac_nan > 0 and frac_nan < 0.05:
        valid_idx = np.where(~nan_mask)[0]
        latent_mu = latent_mu[valid_idx]
        # We must also remember to drop corresponding indices later; here, we'll return outlier indices relative to original ordering.
    elif frac_nan >= 0.05:
        # In production, you might re-run VAE training; here, we simply drop all NaNs.
        valid_idx = np.where(~nan_mask)[0]
        latent_mu = latent_mu[valid_idx]
    outlier_idx = np.where(recon_errors > threshold)[0]
    return latent_mu, recon_errors, outlier_idx, threshold

#####################################
# ROBUST DP-GMM CLUSTERING FUNCTION
#####################################
def robust_dp_gmm_clustering(latent_mu, outlier_idx, alpha=1.0, max_components=9, max_retries=3):
    """
    Clusters latent representations (excluding outliers) using BayesianGaussianMixture with DP prior.
    Retries with different init parameters if convergence warnings occur.
    Returns:
      - labels_all: cluster assignments for all samples (outliers get assigned to a special cluster)
      - n_clusters: total clusters (max 9 from DP-GMM + 1 extra for outliers)
      - bgm: the fitted BayesianGaussianMixture model.
    """
    # Exclude outliers for clustering
    if len(outlier_idx) > 0:
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
        inlier_mask[outlier_idx] = False
        data_cluster = latent_mu[inlier_mask]
    else:
        data_cluster = latent_mu
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
    
    attempts = 0
    while attempts < max_retries:
        try:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter("always")
                bgm = BayesianGaussianMixture(
                    n_components=max_components,
                    weight_concentration_prior=alpha,
                    weight_concentration_prior_type='dirichlet_process',
                    covariance_type='full',
                    init_params='kmeans' if attempts % 2 == 0 else 'random',
                    max_iter=100 * (attempts+1),
                    random_state=42
                )
                labels_inliers = bgm.fit_predict(data_cluster)
                if any("ConvergenceWarning" in str(warn.message) for warn in w):
                    attempts += 1
                    continue
                break
        except ValueError as e:
            # Check if NaN is present; if so, drop NaNs if <5%
            if "NaN" in str(e):
                nan_mask = np.isnan(data_cluster).any(axis=1)
                frac_nan = np.mean(nan_mask)
                if frac_nan < 0.05:
                    data_cluster = data_cluster[~nan_mask]
                    continue
                else:
                    raise e
            else:
                attempts += 1
    else:
        print("DP-GMM did not converge after several attempts. Proceeding with current initialization.")
    
    unique_labels = np.unique(labels_inliers)
    label_mapping = {old: new for new, old in enumerate(unique_labels)}
    relabeled_inliers = np.array([label_mapping[l] for l in labels_inliers])
    n_clusters = len(unique_labels)
    labels_all = -1 * np.ones(latent_mu.shape[0], dtype=int)
    labels_all[inlier_mask] = relabeled_inliers
    # Assign outliers to extra cluster
    if len(outlier_idx) > 0:
        labels_all[outlier_idx] = n_clusters
        n_clusters += 1
    return labels_all, n_clusters, bgm

#####################################
# MERGE CLUSTERS WITH SAME ARTIFACT TYPE
#####################################
def merge_clusters_by_artifact(cluster_stats, labels_all):
    """
    Given cluster statistics (dict mapping cluster_id -> stats including 'assigned_artifact')
    merge clusters that share the same artifact type.
    Returns a mapping: original cluster ids -> merged artifact label.
    """
    mapping = {}
    merged = {}
    for cid, stats in cluster_stats.items():
        art = stats.get("assigned_artifact", "")
        if art == "":
            continue
        # If artifact already assigned to another cluster, merge them.
        if art in merged:
            mapping[cid] = merged[art]
        else:
            mapping[cid] = cid
            merged[art] = cid
    # Remap labels_all using mapping: if cluster id not in mapping, leave as is.
    new_labels = np.array([mapping.get(int(l), int(l)) for l in labels_all])
    return new_labels, merged

#####################################
# ANALYZE CLUSTERS AND ASSIGN ARTIFACT LABELS
#####################################
def analyze_clusters_and_label(df, labels_all, feature_cols, artifact_labels=None):
    """
    Computes cluster statistics and assigns an artifact type to each cluster.
    Here we use a placeholder: for each cluster, we compute the mean and variance of the features,
    then assign an artifact type based on a simple heuristic.
    Returns a dict mapping cluster id to statistics.
    Also updates the DataFrame's "coarse_class_labels" column for detections from classes "G" or "GS".
    """
    df["cluster_id"] = labels_all
    clusters = np.unique(labels_all)
    cluster_stats = {}
    
    global_means = df[feature_cols].mean()
    global_stds = df[feature_cols].std() + 1e-8
    
    # Placeholder heuristic: compute sum of absolute z-scores for each cluster's mean.
    # Then map the aggregate z-score to an artifact type by dividing by a constant.
    artifact_types = [
        "Azimuth Beamforming Error", "Multipath", "Multitarget", "Doppler Artifact",
        "Azimuth Sidelobe", "Monopulse Error", "Elevation-Azimuth Coupling", "NACOM Error",
        "Interference", "Absolute Clutter"
    ]
    for cid in clusters:
        cluster_data = df[df["cluster_id"] == cid]
        size = len(cluster_data)
        if size == 0:
            continue
        feat_means = cluster_data[feature_cols].mean()
        z_scores = (feat_means - global_means) / global_stds
        agg_z = z_scores.abs().sum()
        # Map agg_z to an artifact type index from 0 to len(artifact_types)-1.
        # For example, artifact index = int(agg_z / 10) % len(artifact_types)
        artifact_index = int(agg_z / 10) % len(artifact_types)
        assigned_artifact = artifact_types[artifact_index]
        cluster_stats[int(cid)] = {
            "size": int(size),
            "feature_means": feat_means.to_dict(),
            "feature_z_scores": z_scores.to_dict(),
            "aggregate_z_score": float(agg_z),
            "assigned_artifact": assigned_artifact,
            "artifact_probability": round(100 * (1.0 / (artifact_index + 1)), 2)  # dummy probability value %
        }
        # Update DataFrame labels: for a detection in class "G", label becomes "G"+(artifact index+1)
        # Similarly for "GS".
        orig_class = df.iloc[0]["labels"][0]  # assuming first character of label is class indicator ("G" or "GS")
        new_label = f"{orig_class}{artifact_index+1}"
        df.loc[df["cluster_id"] == cid, "coarse_class_labels"] = new_label
    return cluster_stats

#####################################
# PLOTTING FUNCTION (CUMULATIVE SCATTER PLOT)
#####################################
def save_scatter_plot(df, test_name, output_plot_folder):
    """
    Creates a cumulative scatter plot of all detections (across all cycles).
    X-axis: distY, Y-axis: distX with limits X: [-100,100], Y: [-1,160].
    Uses distinct color+marker combinations per artifact label.
    Ground truth is overlaid (magenta markers+lines, marker size 1, line width 1).
    Legend text includes artifact label and the detection count with artifact probability in % (2 decimal points).
    Legends are off by default.
    Saves the plot as an HTML file.
    """
    # Define a mapping for artifact labels to distinct color+marker.
    # For "TT" and "SE", use fixed styles.
    style_map = {
        "TT": {"color": "red", "symbol": "circle", "opacity": 1.0},
        "SE": {"color": "grey", "symbol": "circle", "opacity": 0.3}
    }
    # For artifact labels from "G" and "GS", assign distinct styles.
    artifact_types = [
        "Azimuth Beamforming Error", "Multipath", "Multitarget", "Doppler Artifact",
        "Azimuth Sidelobe", "Monopulse Error", "Elevation-Azimuth Coupling", "NACOM Error",
        "Interference", "Absolute Clutter"
    ]
    # Predefine a palette of distinct colors and marker symbols.
    palette = ["orange", "darkorange", "tomato", "orangered", "coral", 
               "sandybrown", "goldenrod", "peru", "chocolate", "darkgoldenrod",
               "blue", "mediumblue", "royalblue", "dodgerblue", "deepskyblue",
               "cornflowerblue", "skyblue", "steelblue", "slateblue", "darkblue"]
    marker_symbols = ["circle", "square", "diamond", "cross", "x", "triangle-up", "triangle-down", "pentagon", "hexagon", "star"]
    
    # We'll construct a style map for labels that start with "G" or "GS".
    unique_labels = np.sort(df["coarse_class_labels"].unique())
    for label in unique_labels:
        if label.startswith("G") or label.startswith("GS"):
            # Extract a number from label if possible
            try:
                num = int(''.join(filter(str.isdigit, label)))
            except:
                num = 0
            # Use a combination from palette and marker_symbols (ensure distinctiveness)
            color = palette[num % len(palette)]
            symbol = marker_symbols[num % len(marker_symbols)]
            style_map[label] = {"color": color, "symbol": symbol, "opacity": 1.0}
    
    # Create traces for each artifact label (all cycles together)
    counts = df["coarse_class_labels"].value_counts().to_dict()
    traces = []
    for label in unique_labels:
        subset = df[df["coarse_class_labels"] == label]
        style = style_map.get(label, {"color": "black", "symbol": "circle", "opacity": 1.0})
        traces.append(go.Scatter(
            x=subset["distY"],
            y=subset["distX"],
            mode='markers',
            marker=dict(size=2, color=style["color"], symbol=style["symbol"], opacity=style["opacity"]),
            name=f"{label} ({counts.get(label, 0)})",
            showlegend=False
        ))
    
    # Ground truth: assume df has "distX" and "distY" columns (merged from gnd.ftr)
    gt_trace = go.Scatter(
        x=df["distY"],
        y=df["distX"],
        mode='markers+lines',
        marker=dict(size=1, color="magenta"),
        line=dict(width=1, color="magenta"),
        name=f"Ground Truth ({len(df)})",
        showlegend=False
    )
    traces.append(gt_trace)
    
    fig = go.Figure(data=traces)
    fig.update_layout(
        title=f"Cumulative Artifact Scatter Plot - {test_name}",
        xaxis=dict(title="distY", range=[-100, 100]),
        yaxis=dict(title="distX", range=[-1, 160]),
        showlegend=False
    )
    output_plot_path = os.path.join(output_plot_folder, f"{test_name}_scatter.html")
    fig.write_html(output_plot_path)
    print(f"Scatter plot saved as {output_plot_path}")

#####################################
# MAIN PROCESSING FUNCTION FOR MULTIPLE TESTCASES
#####################################
def process_testcases(root_folder, radar_folder, gnd_folder, output_radar_folder, output_stats_folder, output_plot_folder):
    """
    Processes multiple test cases.
    
    Arguments:
      - root_folder: folder containing radar ftr files named as testcase_name.ftr.
      - gnd_folder: separate path containing ftr files named as testcase_name_gnd.ftr.
      - output_radar_folder: folder where updated radar ftr files are saved (testcase_name.ftr).
      - output_stats_folder: folder where cluster statistics JSON files are saved.
      - output_plot_folder: folder where cumulative scatter plots (HTML) are saved.
    """
    os.makedirs(output_radar_folder, exist_ok=True)
    os.makedirs(output_stats_folder, exist_ok=True)
    os.makedirs(output_plot_folder, exist_ok=True)
    
    test_files = [f for f in os.listdir(root_folder) if f.endswith(".ftr")]
    for test_file in test_files:
        test_name = os.path.splitext(test_file)[0]
        print(f"Processing test case: {test_name}")
        test_path = os.path.join(root_folder, test_file)
        
        # Load data: radar from root_folder; ground truth from gnd_folder.
        dataset, clustering_feature_cols, dfRadar, dfGND = load_and_preprocess_data(os.path.join(root_folder, test_file), root_folder, gnd_folder)
        
        # Process classes "G" and "GS" separately
        # Only process detections whose coarse_class_labels are "G" or "GS".
        df_G, cluster_stats_G = process_class(dfRadar, "G")
        df_GS, cluster_stats_GS = process_class(dfRadar, "GS")
        
        # Update the original DataFrame with new labels for "G" and "GS"
        dfRadar.update(df_G)
        dfRadar.update(df_GS)
        
        # Save updated radar file in output_radar_folder (named as test_name.ftr)
        output_radar_path = os.path.join(output_radar_folder, f"{test_name}.ftr")
        dfRadar.to_feather(output_radar_path)
        print(f"Updated radar file saved to: {output_radar_path}")
        
        # Combine cluster statistics for "G" and "GS" and save as JSON
        all_cluster_stats = {"G": cluster_stats_G, "GS": cluster_stats_GS}
        output_stats_path = os.path.join(output_stats_folder, f"{test_name}_cluster_stats.json")
        with open(output_stats_path, 'w') as f:
            json.dump(all_cluster_stats, f, indent=2)
        print(f"Cluster statistics saved to: {output_stats_path}")
        
        # Generate cumulative scatter plot for this test case
        save_scatter_plot(dfRadar, clustering_feature_cols, test_name, output_plot_folder)
        
        # Clean up GPU memory
        torch.cuda.empty_cache()

#####################################
# MAIN ENTRY POINT
#####################################
if __name__ == '__main__':
    # Define paths (adjust these paths as needed)
    root_folder = "./testcases_root"         # radar ftr files (named as testcase_name.ftr)
    gnd_folder = "./gnd_folder"                # ground truth ftr files (named as testcase_name_gnd.ftr)
    output_radar_folder = "./updated_radar"    # folder for updated radar ftr files
    output_stats_folder = "./cluster_stats"    # folder for cluster statistics JSON files
    output_plot_folder = "./plots"             # folder for cumulative scatter plots
    
    process_testcases(root_folder, radar_folder, gnd_folder, output_radar_folder, output_stats_folder, output_plot_folder)
