import os
import json
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import torch
from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
from torch.nn import functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import BayesianGaussianMixture
from tqdm import tqdm
from scipy.stats import multivariate_normal

#####################################
# DEVICE CONFIGURATION
#####################################
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

#####################################
# DATA LOADING & PREPROCESSING
#####################################
def load_and_preprocess_data(test_path, radar_folder, gnd_folder):
    """
    Loads radar.ftr and corresponding ground truth ftr from given paths.
    Radar file is named: <testcase>.ftr in radar_folder.
    Ground truth file is named: <testcase>_gnd.ftr in gnd_folder.
    Returns:
      - PyTorch dataset (normalized features)
      - feature_cols list,
      - optional artifact_labels (if any),
      - scaler parameters (mean, scale),
      - radar DataFrame, and ground truth DataFrame.
    """
    testcase_name = os.path.basename(test_path)
    radar_file = os.path.join(test_path, radar_folder, f"{testcase_name}.ftr")
    gnd_file = os.path.join(gnd_folder, f"{testcase_name}_gnd.ftr")
    
    df_radar = pd.read_feather(radar_file)
    df_gnd = pd.read_feather(gnd_file)
    
    # Define the feature columns (all quality and kinematics signals)
    feature_cols = [
        "Range", "Azimuth", "VrelRad", "SNR", "RCS",
        "aa", "bb", "cc", "dd", "ee", "ff", "gg",
        "RangeVar", "Azimuth var", "VrelRad var", "Elev Var",
        "hh", "jj", "kk", "Elevation Angle", "beamforming model type"
    ]
    data = df_radar[feature_cols].values.astype(np.float32)
    scaler = StandardScaler()
    data_norm = scaler.fit_transform(data)
    tensor_data = torch.tensor(data_norm, dtype=torch.float32)
    dataset = TensorDataset(tensor_data)
    
    artifact_labels = None
    if "coarse_cluster_labels" in df_radar.columns:
        artifact_labels = df_radar["coarse_cluster_labels"].astype(str).fillna("").values
        
    return dataset, feature_cols, artifact_labels, scaler.mean_, scaler.scale_, df_radar, df_gnd

#####################################
# VIB+VAE MODEL DEFINITION (Shared Encoder)
#####################################
class VIBVAE(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim=128):
        super(VIBVAE, self).__init__()
        # Encoder network
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_mean = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        # Decoder network
        self.fc_dec1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_dec2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, input_dim)
    
    def encode(self, x):
        h = F.relu(self.fc1(x))
        h = F.relu(self.fc2(h))
        mu = self.fc_mean(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = F.relu(self.fc_dec1(z))
        h = F.relu(self.fc_dec2(h))
        recon = self.fc_out(h)
        return recon
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar

def vae_loss(x, recon, mu, logvar):
    # Reconstruction loss: MSE (sum over features)
    recon_loss = F.mse_loss(recon, x, reduction='sum')
    # KL divergence loss
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_loss

def train_vae(model, dataset, batch_size=1024, lr=1e-3, epochs=10, device=device):
    model.to(device)
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
    for epoch in tqdm(range(epochs), desc="Training VAE on GPU"):
        total_loss = 0.0
        for batch in data_loader:
            x = batch[0].to(device)
            optimizer.zero_grad()
            recon, mu, logvar = model(x)
            loss = vae_loss(x, recon, mu, logvar)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        # Uncomment to print progress:
        # print(f"Epoch {epoch+1}/{epochs}, Loss per sample: {total_loss/len(dataset):.4f}")
    model.to('cpu')
    return model

#####################################
# ENCODE DATA & DETECT OUTLIERS (Using Reconstruction Error)
#####################################
def encode_data_and_find_outliers(model, dataset, outlier_percentile=99):
    model.eval()
    data_loader = DataLoader(dataset, batch_size=1024, shuffle=False, pin_memory=True)
    latent_list, recon_err_list = [], []
    with torch.no_grad():
        for batch in data_loader:
            x = batch[0]
            recon, mu, logvar = model(x)
            latent_list.append(mu.numpy())
            batch_err = ((recon.numpy() - x.numpy())**2).mean(axis=1)
            recon_err_list.append(batch_err)
    latent_mu = np.concatenate(latent_list, axis=0)
    recon_errors = np.concatenate(recon_err_list, axis=0)
    threshold = np.percentile(recon_errors, outlier_percentile)
    outlier_idx = np.where(recon_errors > threshold)[0]
    return latent_mu, recon_errors, outlier_idx, threshold

#####################################
# DP-GMM CLUSTERING FUNCTION (Using BayesianGaussianMixture)
#####################################
def dp_gmm_clustering(latent_mu, outlier_idx, alpha=1.0, max_components=20, max_artifacts=9):
    # Exclude outliers for clustering
    if len(outlier_idx) > 0:
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
        inlier_mask[outlier_idx] = False
        data_cluster = latent_mu[inlier_mask]
    else:
        data_cluster = latent_mu
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
    bgm = BayesianGaussianMixture(
        n_components=max_components,
        weight_concentration_prior=alpha,
        weight_concentration_prior_type='dirichlet_process',
        covariance_type='full',
        init_params='kmeans',
        max_iter=100,
        random_state=0
    )
    labels_inliers = bgm.fit_predict(data_cluster)
    unique_labels = np.unique(labels_inliers)
    # Relabel clusters to be contiguous
    label_mapping = {old: new for new, old in enumerate(unique_labels)}
    relabeled_inliers = np.array([label_mapping[l] for l in labels_inliers])
    n_clusters = len(unique_labels)
    # Limit clusters to max_artifacts (max 9) if n_clusters > max_artifacts:
    if n_clusters > max_artifacts:
        # Merge clusters with smallest sizes until we have max_artifacts
        counts = {l: np.sum(relabeled_inliers == l) for l in np.unique(relabeled_inliers)}
        # Sort clusters by count
        sorted_clusters = sorted(counts.items(), key=lambda x: x[1])
        # For simplicity, we reassign clusters with smallest counts to the nearest larger cluster
        # This is a placeholder: in practice, one might use hierarchical clustering to merge.
        for old_label, cnt in sorted_clusters[:n_clusters - max_artifacts]:
            # Reassign all points in this cluster to the cluster with highest count
            target_label = max(counts, key=counts.get)
            relabeled_inliers[relabeled_inliers == old_label] = target_label
        unique_labels = np.unique(relabeled_inliers)
        n_clusters = len(unique_labels)
        # Re-map to contiguous integers
        label_mapping = {old: new for new, old in enumerate(unique_labels)}
        relabeled_inliers = np.array([label_mapping[l] for l in relabeled_inliers])
    labels_all = -1 * np.ones(latent_mu.shape[0], dtype=int)
    labels_all[inlier_mask] = relabeled_inliers
    if len(outlier_idx) > 0:
        labels_all[outlier_idx] = n_clusters  # assign outliers to extra cluster
        n_clusters += 1
    return labels_all, n_clusters, bgm

#####################################
# CLUSTER STATISTICS & ARTIFACT MAPPING
#####################################
artifact_types = [
    "Azimuth Beamforming Error", "Multipath", "Multitarget", "Doppler Artifact",
    "Azimuth Sidelobe", "Monopulse Error", "Elevation-Azimuth Coupling", "NACOM Error",
    "Interference", "Absolute Clutter"  # we expect max 10 artifact types overall
]

def analyze_clusters_and_label(df, labels_all, feature_cols, artifact_labels=None):
    """
    Computes statistics for each cluster and assigns an artifact type based on the statistics.
    The mapping is done probabilistically (placeholder function).
    Updates df["coarse_class_labels"] for detections in classes "G" or "GS".
    Returns cluster statistics as a dict.
    """
    df["cluster_id"] = labels_all
    clusters = np.unique(labels_all)
    stats_dict = {}
    global_means = df[feature_cols].mean()
    global_stds = df[feature_cols].std() + 1e-8
    for cid in clusters:
        cluster_data = df[df["cluster_id"] == cid]
        size = len(cluster_data)
        if size == 0:
            continue
        feat_means = cluster_data[feature_cols].mean()
        z_scores = (feat_means - global_means) / global_stds
        aggregate_z = float(z_scores.abs().sum())
        # Placeholder: assign artifact type based on aggregate_z modulo length of artifact_types
        artifact_idx = int(aggregate_z) % len(artifact_types)
        assigned_artifact = artifact_types[artifact_idx]
        stats_dict[int(cid)] = {
            "size": int(size),
            "feature_means": feat_means.to_dict(),
            "feature_z_scores": z_scores.to_dict(),
            "aggregate_z_score": aggregate_z,
            "assigned_artifact": assigned_artifact,
            "cluster_probability": round((size / len(df)) * 100, 2)  # percentage of detections
        }
        # Final label: original coarse class ("G" or "GS") + artifact number (artifact_idx+1)
        prefix = df["labels"].iloc[0][0]  # e.g. "G" or "G" if class "G", or "G" for "GS" as well (we assume first character distinguishes)
        final_label = f"{prefix}{artifact_idx+1}"
        df.loc[df["cluster_id"] == cid, "coarse_class_labels"] = final_label
    return stats_dict

#####################################
# SAVE OUTPUTS (Radar File and Cluster Stats JSON)
#####################################
def save_outputs(df, cluster_stats, test_name, output_radar_folder, output_stats_folder):
    out_radar_path = os.path.join(output_radar_folder, f"{test_name}.ftr")
    df.to_feather(out_radar_path)
    out_stats_path = os.path.join(output_stats_folder, f"{test_name}_cluster_stats.json")
    with open(out_stats_path, 'w') as f:
        json.dump(cluster_stats, f, indent=2)
    print(f"Saved updated radar file: {out_radar_path}")
    print(f"Saved cluster statistics: {out_stats_path}")

#####################################
# PLOTTING FUNCTION (CUMULATIVE SCATTER PLOT)
#####################################
def save_scatter_plot(df, test_name, output_plot_folder):
    """
    Creates a cumulative scatter plot using "distY" (x-axis) and "distX" (y-axis).
    Detections are colored and marked by artifact label from "coarse_class_labels".
    Ground truth points are overlaid as magenta markers+lines.
    Legend entries display the label and cluster probability in % (2 decimal points).
    Plot limits: X axis [-100, 100], Y axis [-1, 160]. Legends are off by default.
    """
    # Define colors and markers: fixed for "TT" and "SE", and distinct for others.
    fixed_styles = {"TT": {"color": "red", "symbol": "circle", "opacity": 1.0},
                    "SE": {"color": "grey", "symbol": "circle", "opacity": 0.3}}
    # Define a palette for G and GS artifact clusters.
    artifact_palette = [
        {"color": "orange", "symbol": "square"},
        {"color": "darkorange", "symbol": "diamond"},
        {"color": "tomato", "symbol": "triangle-up"},
        {"color": "orangered", "symbol": "triangle-down"},
        {"color": "coral", "symbol": "x"},
        {"color": "sandybrown", "symbol": "cross"},
        {"color": "goldenrod", "symbol": "pentagon"},
        {"color": "peru", "symbol": "hexagon"},
        {"color": "chocolate", "symbol": "star"}
    ]
    
    def get_style(label):
        if label in fixed_styles:
            return fixed_styles[label]
        elif label.startswith("G") or label.startswith("GS"):
            try:
                # Extract artifact number (assumes label like "G3" or "GS5")
                num = int(''.join(filter(str.isdigit, label)))
            except:
                num = 1
            style = artifact_palette[(num - 1) % len(artifact_palette)]
            return style
        else:
            return {"color": "black", "symbol": "circle"}
    
    # Count detections per coarse_class_labels (final labels)
    counts = df["coarse_class_labels"].value_counts().to_dict()
    
    traces = []
    unique_labels = np.sort(df["coarse_class_labels"].unique())
    for label in unique_labels:
        subset = df[df["coarse_class_labels"] == label]
        style = get_style(label)
        prob_text = ""
        # If available, get cluster probability from cluster_stats (if label exists in stats)
        # Here, assume label is like "G3": the number 3 corresponds to cluster id 2.
        try:
            num = int(''.join(filter(str.isdigit, label))) - 1
            # Search in cluster_stats if available
            # For demonstration, we assume each detection in the cluster shares a probability stored in a column "cluster_prob"
            if "cluster_prob" in subset.columns:
                p_val = subset["cluster_prob"].iloc[0]  # take the first sample's probability
                prob_text = f" ({p_val:.2f}%)"
        except:
            pass
        traces.append(go.Scatter(
            x=subset["distY"],  # x-axis
            y=subset["distX"],  # y-axis
            mode='markers',
            marker=dict(size=2, color=style["color"], symbol=style["symbol"], opacity=style.get("opacity", 1.0)),
            name=f"{label}{prob_text}",
            showlegend=False  # legends off by default
        ))
    
    # Ground truth: assume df has "distX" and "distY"
    gt_trace = go.Scatter(
        x=df["distY"],
        y=df["distX"],
        mode='markers+lines',
        marker=dict(size=1, color="magenta"),
        line=dict(width=1, color="magenta"),
        name=f"Ground Truth ({len(df)})",
        showlegend=False
    )
    traces.append(gt_trace)
    
    fig = go.Figure(data=traces)
    fig.update_layout(title=f"Cumulative Artifact Scatter Plot - {test_name}",
                      xaxis=dict(title="distY", range=[-100, 100]),
                      yaxis=dict(title="distX", range=[-1, 160]),
                      showlegend=False)
    output_plot_path = os.path.join(output_plot_folder, f"{test_name}_scatter.html")
    fig.write_html(output_plot_path)
    print(f"Scatter plot saved as {output_plot_path}")

#####################################
# PROCESS FUNCTION FOR CLASS "G" and "GS"
#####################################
def process_class(dfRadar, class_label):
    """
    Processes detections for class "G" or "GS" using VIB+DP-GMMVAE.
    Returns updated DataFrame for that class and cluster statistics.
    """
    df_class = dfRadar[dfRadar["labels"] == class_label].copy()
    feature_cols = [
        "Range", "Azimuth", "VrelRad", "SNR", "RCS",
        "aa", "bb", "cc", "dd", "ee", "ff", "gg",
        "RangeVar", "Azimuth var", "VrelRad var", "Elev Var",
        "hh", "jj", "kk", "Elevation Angle", "beamforming model type"
    ]
    X = df_class[feature_cols].values.astype(np.float32)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    dataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32))
    
    # Train VIB+VAE model on this class data (batch-wise on GPU)
    model = VIBVAE(input_dim=X_scaled.shape[1], latent_dim=16, hidden_dim=128)
    model = train_vae(model, dataset, batch_size=1024, lr=1e-3, epochs=20, device=device)
    
    # Encode all data and compute reconstruction error for outlier detection
    latent_mu, recon_errors, outlier_idx, thr = encode_data_and_find_outliers(model, dataset, outlier_percentile=99)
    print(f"[{class_label}] VAE reconstruction error threshold: {thr:.4f} with {len(outlier_idx)} outliers")
    
    # Split data into two groups:
    # Group 1: Reconstructable detections (reconstruction error <= thr, i.e. >=75% accuracy assumed)
    # Group 2: Non-reconstructable detections -> assign as "Absolute Clutter"
    total = latent_mu.shape[0]
    reconstructable_idx = np.where(recon_errors <= thr)[0]
    non_reconstructable_idx = np.where(recon_errors > thr)[0]
    
    # Initialize final label array for all detections in this class.
    final_labels = np.array([None] * total)
    # Non-reconstructable detections are assigned a fixed label, e.g., if class is "G", label as "G_clutter"
    for idx in non_reconstructable_idx:
        final_labels[idx] = f"{class_label}_clutter"
    
    # For reconstructable detections, cluster latent space using DP-GMM
    if len(reconstructable_idx) > 0:
        latent_recon = latent_mu[reconstructable_idx]
        labels_recon, n_clusters, bgm_model = dp_gmm_clustering(latent_recon, np.array([]), alpha=1.0, max_components=20, max_artifacts=9)
        # Now, ensure that each artifact type appears only once.
        # Compute basic cluster statistics on the reconstructable subset.
        df_recon = df_class.iloc[reconstructable_idx].copy()
        # We add cluster assignments temporarily.
        df_recon["cluster_id"] = labels_recon
        # Analyze clusters and assign artifact types based on statistical analysis.
        cluster_stats = analyze_clusters_and_label(df_recon, labels_recon, feature_cols, artifact_labels=None)
        # For each reconstructable detection, assign final label: for class "G", label becomes "G" + (artifact index + 1)
        for i, lab in enumerate(labels_recon):
            # Use the assigned artifact from cluster_stats:
            assigned = cluster_stats.get(int(lab), {}).get("assigned_artifact", "")
            # For simplicity, we use the cluster number (lab+1)
            final_labels[reconstructable_idx[i]] = f"{class_label}{lab+1}"
    else:
        cluster_stats = {}
    df_class.loc[:, "coarse_class_labels"] = final_labels
    return df_class, cluster_stats

#####################################
# MAIN PROCESSING FUNCTION FOR MULTIPLE TESTCASES
#####################################
def process_testcases(root_folder, radar_folder, gnd_folder, output_radar_folder, output_stats_folder, output_plot_folder):
    """
    Processes multiple test cases.
    - Root folder contains radar ftr files named as <testcase>.ftr.
    - Ground truth files are in a separate gnd_folder with filenames <testcase>_gnd.ftr.
    - Updated radar files are saved in output_radar_folder as <testcase>.ftr.
    - Cluster statistics JSON is saved in output_stats_folder.
    - Cumulative scatter plots are saved in output_plot_folder.
    """
    os.makedirs(output_radar_folder, exist_ok=True)
    os.makedirs(output_stats_folder, exist_ok=True)
    os.makedirs(output_plot_folder, exist_ok=True)
    
    testcases = [d for d in os.listdir(root_folder) if os.path.isfile(os.path.join(root_folder, d)) and d.endswith(".ftr")]
    for testcase_file in testcases:
        test_name = os.path.splitext(testcase_file)[0]
        print(f"Processing test case: {test_name}")
        # For radar file, path is root_folder/<testcase>.ftr
        # For ground truth, path is gnd_folder/<testcase>_gnd.ftr
        dfRadar = pd.read_feather(os.path.join(root_folder, testcase_file))
        dfGND = pd.read_feather(os.path.join(gnd_folder, f"{test_name}_gnd.ftr"))
        
        # Process "G" and "GS" separately
        df_G, cluster_stats_G = process_class(dfRadar, "G")
        df_GS, cluster_stats_GS = process_class(dfRadar, "GS")
        # Update dfRadar with new labels for "G" and "GS"
        dfRadar.update(df_G)
        dfRadar.update(df_GS)
        
        # Save updated radar file
        out_radar_path = os.path.join(output_radar_folder, f"{test_name}.ftr")
        dfRadar.to_feather(out_radar_path)
        print(f"Updated radar file saved to: {out_radar_path}")
        
        # Combine cluster statistics for "G" and "GS" and save as JSON
        all_cluster_stats = {"G": cluster_stats_G, "GS": cluster_stats_GS}
        out_stats_path = os.path.join(output_stats_folder, f"{test_name}_cluster_stats.json")
        with open(out_stats_path, 'w') as f:
            json.dump(all_cluster_stats, f, indent=2)
        print(f"Cluster statistics saved to: {out_stats_path}")
        
        # Generate cumulative scatter plot.
        save_scatter_plot(dfRadar, test_name, output_plot_folder)
        
        # Clean up memory for this test case.
        del dfRadar, dfGND
        torch.cuda.empty_cache()

#####################################
# MAIN ENTRY POINT
#####################################
if __name__ == '__main__':
    # Define paths:
    root_folder = "./radar_files"            # Folder containing radar.ftr files (<testcase>.ftr)
    gnd_folder = "./ground_truth_files"        # Folder containing ground truth ftr files (<testcase>_gnd.ftr)
    output_radar_folder = "./updated_radar"    # Folder to save updated radar.ftr files
    output_stats_folder = "./cluster_stats"    # Folder to save cluster statistics JSON
    output_plot_folder = "./plots"             # Folder to save cumulative scatter plots
    
    process_testcases(root_folder, "", gnd_folder, output_radar_folder, output_stats_folder, output_plot_folder)
